name: Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    paths:
      - 'src/runtime/**'
      - 'src/tooling/WallstopStudios.NovaSharp.Benchmarks/**'
      - '.github/workflows/benchmarks.yml'
  workflow_dispatch:
    inputs:
      alert_threshold:
        description: 'Percentage threshold for performance regression alerts'
        required: false
        default: '115'
      fail_on_alert:
        description: 'Fail the workflow if regression is detected'
        required: false
        default: 'true'

permissions:
  contents: write
  pull-requests: write

jobs:
  benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v5
        with:
          dotnet-version: '8.0.x'

      - name: Restore dependencies
        run: |
          # Try locked-mode first, fall back to regular restore if lock files are out of sync
          dotnet restore src/NovaSharp.sln --locked-mode || dotnet restore src/NovaSharp.sln

      - name: Restore tools
        run: dotnet tool restore

      - name: Build solution
        run: dotnet build src/NovaSharp.sln -c Release --no-restore

      - name: Run benchmarks
        id: run_benchmarks
        continue-on-error: true
        run: |
          set -euo pipefail
          echo "Running benchmarks..."
          dotnet run --project src/tooling/WallstopStudios.NovaSharp.Benchmarks/WallstopStudios.NovaSharp.Benchmarks.csproj \
            -c Release --no-build -- \
            --filter "*RuntimeBenchmarks*" \
            --exporters json
          echo "Benchmark run completed"
          # List what was created for debugging
          echo "Contents of BenchmarkDotNet.Artifacts:"
          find BenchmarkDotNet.Artifacts -type f -name "*.json" 2>/dev/null || echo "No JSON files found"
          ls -la BenchmarkDotNet.Artifacts/ 2>/dev/null || echo "BenchmarkDotNet.Artifacts directory not found"
        env:
          DOTNET_ROLL_FORWARD: Major
          # Skip Performance.md update in CI to avoid git conflicts
          NOVASHARP_SKIP_PERFORMANCE_DOC: '1'

      - name: Find and validate benchmark results
        id: find_result
        continue-on-error: true
        run: |
          set -uo pipefail
          
          # Look for the benchmark result files
          echo "Searching for benchmark results..."
          
          # BenchmarkDotNet creates results in BenchmarkDotNet.Artifacts/results/
          if [ ! -d "BenchmarkDotNet.Artifacts" ]; then
            echo "::warning::BenchmarkDotNet.Artifacts directory not found"
            echo "has_results=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          
          # Find the full report JSON (preferred) or any JSON with benchmark data
          result_file=""
          
          # First try: full compressed report
          result_file=$(find BenchmarkDotNet.Artifacts -name "*-report-full-compressed.json" -type f 2>/dev/null | head -1 || true)
          
          # Second try: full report (uncompressed)
          if [ -z "$result_file" ]; then
            result_file=$(find BenchmarkDotNet.Artifacts -name "*-report-full.json" -type f 2>/dev/null | head -1 || true)
          fi
          
          # Third try: any report with RuntimeBenchmarks
          if [ -z "$result_file" ]; then
            result_file=$(find BenchmarkDotNet.Artifacts -name "*RuntimeBenchmarks*.json" -type f 2>/dev/null | head -1 || true)
          fi
          
          # Fourth try: any report JSON
          if [ -z "$result_file" ]; then
            result_file=$(find BenchmarkDotNet.Artifacts -name "*-report*.json" -type f 2>/dev/null | head -1 || true)
          fi
          
          if [ -z "$result_file" ]; then
            echo "::warning::No benchmark result JSON file found in BenchmarkDotNet.Artifacts"
            echo "Directory contents:"
            find BenchmarkDotNet.Artifacts -type f 2>/dev/null || echo "Empty or missing"
            echo "has_results=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          
          echo "Found benchmark result: $result_file"
          echo "result_file=$result_file" >> "$GITHUB_OUTPUT"
          echo "has_results=true" >> "$GITHUB_OUTPUT"

      - name: Transform to benchmark-action format
        id: transform
        if: steps.find_result.outputs.has_results == 'true'
        run: |
          set -euo pipefail
          python3 << 'PYTHON_SCRIPT'
          import json
          import sys
          from pathlib import Path
          import os

          # Find the benchmark results
          artifacts_dir = Path("BenchmarkDotNet.Artifacts")
          
          if not artifacts_dir.exists():
              print("::error::BenchmarkDotNet.Artifacts directory not found")
              sys.exit(1)
          
          # Search for result files in priority order
          result_files = []
          
          # Priority 1: Full compressed report
          result_files = list(artifacts_dir.rglob("*-report-full-compressed.json"))
          
          # Priority 2: Full report
          if not result_files:
              result_files = list(artifacts_dir.rglob("*-report-full.json"))
          
          # Priority 3: Any report with RuntimeBenchmarks
          if not result_files:
              result_files = [f for f in artifacts_dir.rglob("*.json") if "RuntimeBenchmarks" in f.name]
          
          # Priority 4: Any report JSON
          if not result_files:
              result_files = [f for f in artifacts_dir.rglob("*-report*.json")]
          
          if not result_files:
              print("::error::No benchmark JSON files found")
              print(f"Contents of {artifacts_dir}:")
              for p in artifacts_dir.rglob("*"):
                  print(f"  {p}")
              sys.exit(1)
          
          result_file = result_files[0]
          print(f"Processing: {result_file}")
          
          with open(result_file) as f:
              data = json.load(f)
          
          # Transform to github-action-benchmark format
          benchmarks = []
          benchmark_list = data.get("Benchmarks", [])
          
          if not benchmark_list:
              print("::warning::No benchmarks found in result file, creating placeholder")
              # Create a placeholder so the workflow doesn't fail
              benchmarks.append({
                  "name": "NovaSharp.Placeholder",
                  "unit": "ns",
                  "value": 0,
                  "extra": "No benchmark data available"
              })
          else:
              for benchmark in benchmark_list:
                  name = benchmark.get("FullName", benchmark.get("Method", "Unknown"))
                  stats = benchmark.get("Statistics", {})
                  mean_ns = stats.get("Mean", 0)
                  
                  # Handle case where Statistics might be missing
                  if not stats and "Mean" in benchmark:
                      mean_ns = benchmark.get("Mean", 0)
                  
                  # Convert nanoseconds to appropriate unit for readability
                  if mean_ns > 1_000_000_000:
                      value = mean_ns / 1_000_000_000
                      unit = "s"
                  elif mean_ns > 1_000_000:
                      value = mean_ns / 1_000_000
                      unit = "ms"
                  elif mean_ns > 1_000:
                      value = mean_ns / 1_000
                      unit = "μs"
                  else:
                      value = mean_ns
                      unit = "ns"
                  
                  p95 = stats.get("P95", 0)
                  extra = f"P95: {p95 / 1_000:.3f}μs" if p95 else ""
                  
                  benchmarks.append({
                      "name": name,
                      "unit": unit,
                      "value": round(value, 3),
                      "extra": extra
                  })
          
          # Write output
          output_path = Path("benchmark-results.json")
          with open(output_path, "w") as f:
              json.dump(benchmarks, f, indent=2)
          
          print(f"Wrote {len(benchmarks)} benchmarks to {output_path}")
          
          # Verify the file was created
          if not output_path.exists():
              print("::error::Failed to create benchmark-results.json")
              sys.exit(1)
          
          print(f"File size: {output_path.stat().st_size} bytes")
          
          # Also write to step summary
          summary = os.environ.get("GITHUB_STEP_SUMMARY", "")
          if summary:
              with open(summary, "a") as f:
                  f.write("## Benchmark Results\n\n")
                  f.write("| Name | Value | Unit | Extra |\n")
                  f.write("|------|-------|------|-------|\n")
                  for b in benchmarks:
                      f.write(f"| {b['name']} | {b['value']} | {b['unit']} | {b.get('extra', '')} |\n")
                  f.write("\n")
          
          print("Transform completed successfully")
          PYTHON_SCRIPT

      - name: Verify benchmark-results.json exists
        id: verify_results
        if: steps.find_result.outputs.has_results == 'true'
        run: |
          if [ ! -f "benchmark-results.json" ]; then
            echo "::warning::benchmark-results.json was not created"
            echo "has_output=false" >> "$GITHUB_OUTPUT"
            exit 0
          fi
          echo "benchmark-results.json contents:"
          cat benchmark-results.json
          echo ""
          echo "has_output=true" >> "$GITHUB_OUTPUT"

      - name: Set output when no results
        id: set_no_output
        if: steps.find_result.outputs.has_results != 'true'
        run: |
          echo "has_output=false" >> "$GITHUB_OUTPUT"
          echo "::warning::No benchmark results available to store"

      - name: Clean up local changes before branch operations
        run: |
          # The benchmark harness may update docs/Performance.md locally.
          # Discard these changes so git can switch branches cleanly.
          git checkout -- docs/Performance.md 2>/dev/null || true
          # Also reset any other untracked/modified files that could interfere
          git clean -fd 2>/dev/null || true

      - name: Check if gh-pages branch exists
        id: check_gh_pages
        run: |
          if git ls-remote --exit-code --heads origin gh-pages > /dev/null 2>&1; then
            echo "exists=true" >> "$GITHUB_OUTPUT"
            echo "gh-pages branch exists"
          else
            echo "exists=false" >> "$GITHUB_OUTPUT"
            echo "gh-pages branch does not exist"
          fi

      - name: Create gh-pages branch
        if: steps.check_gh_pages.outputs.exists == 'false' && github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          echo "Creating gh-pages branch for benchmark storage..."
          # Create orphan branch with initial benchmark data structure
          git checkout --orphan gh-pages
          git reset --hard
          mkdir -p benchmarks
          echo '[]' > benchmarks/data.js
          echo '# Benchmark Results' > README.md
          echo '' >> README.md
          echo 'This branch contains historical benchmark data for NovaSharp.' >> README.md
          git add benchmarks/data.js README.md
          git -c user.name="github-actions[bot]" -c user.email="github-actions[bot]@users.noreply.github.com" \
            commit -m "Initialize gh-pages branch for benchmark storage"
          git push origin gh-pages
          # Return to original branch
          git checkout -
          echo "gh-pages branch created successfully"

      - name: Check if benchmark results file exists
        id: check_results_file
        run: |
          if [ -f "benchmark-results.json" ]; then
            echo "file_exists=true" >> "$GITHUB_OUTPUT"
            echo "benchmark-results.json exists"
            ls -la benchmark-results.json
          else
            echo "file_exists=false" >> "$GITHUB_OUTPUT"
            echo "::warning::benchmark-results.json does not exist"
          fi

      - name: Store benchmark results
        # Only run if the file physically exists AND gh-pages exists (either pre-existing or just created)
        if: |
          steps.check_results_file.outputs.file_exists == 'true' &&
          (steps.check_gh_pages.outputs.exists == 'true' || (github.event_name == 'push' && github.ref == 'refs/heads/main'))
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: NovaSharp Benchmarks
          tool: customSmallerIsBetter
          output-file-path: benchmark-results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          # Auto-push only on main branch pushes
          auto-push: ${{ github.event_name == 'push' && github.ref == 'refs/heads/main' }}
          # Alert threshold (default 115% = 15% regression)
          alert-threshold: ${{ github.event.inputs.alert_threshold || '115%' }}
          # Fail on alert for PRs, configurable via input
          fail-on-alert: ${{ github.event.inputs.fail_on_alert == 'true' || (github.event_name == 'pull_request') }}
          # Comment on PR with results
          comment-on-alert: true
          # Store data in gh-pages branch under benchmarks directory
          gh-pages-branch: gh-pages
          benchmark-data-dir-path: benchmarks

      - name: Skip benchmark storage (no gh-pages branch or no results)
        if: |
          steps.check_results_file.outputs.file_exists != 'true' ||
          (steps.check_gh_pages.outputs.exists == 'false' && !(github.event_name == 'push' && github.ref == 'refs/heads/main'))
        run: |
          if [ "${{ steps.check_results_file.outputs.file_exists }}" != "true" ]; then
            echo "::warning::Skipping benchmark storage - benchmark-results.json was not created."
          else
            echo "::warning::Skipping benchmark storage - gh-pages branch does not exist."
            echo "The gh-pages branch will be created on the next push to main."
          fi
          echo "Benchmark results are still available in the uploaded artifacts (if any)."

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: benchmark-results
          path: |
            BenchmarkDotNet.Artifacts/
            benchmark-results.json
          if-no-files-found: warn

  # Comparison benchmarks run separately and less frequently
  comparison:
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    needs: benchmark
    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Setup .NET
        uses: actions/setup-dotnet@v5
        with:
          dotnet-version: '8.0.x'

      - name: Restore dependencies
        run: dotnet restore src/NovaSharp.sln

      - name: Restore tools
        run: dotnet tool restore

      - name: Build solution
        run: dotnet build src/NovaSharp.sln -c Release --no-restore

      - name: Run comparison benchmarks
        run: |
          dotnet run --project src/tooling/WallstopStudios.NovaSharp.Comparison/WallstopStudios.NovaSharp.Comparison.csproj \
            -c Release --no-build -- \
            --filter "*" \
            --exporters json
        env:
          DOTNET_ROLL_FORWARD: Major

      - name: Upload comparison artifacts
        uses: actions/upload-artifact@v6
        with:
          name: comparison-results
          path: BenchmarkDotNet.Artifacts/
          if-no-files-found: warn
