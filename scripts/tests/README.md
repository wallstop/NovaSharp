# tests scripts

Utilities that keep the interpreter test metadata in sync now that all fixtures run on
Microsoft.Testing.Platform/TUnit, plus Lua specification parity harnesses.

## Lua Specification Parity Harness

### run-lua-fixtures-parallel.py (Recommended)

- **Purpose:** High-performance parallel runner for comparing Lua fixtures against reference Lua and NovaSharp.
- **Performance:** ~27 fixtures/second with 8 workers (vs ~1/second sequential).
- **Usage:**
  ```bash
  # Run against Lua 5.1 with 8 workers (default: CPU count)
  python3 scripts/tests/run-lua-fixtures-parallel.py --lua-version 5.1 -j 8

  # Full comparison against Lua 5.4
  python3 scripts/tests/run-lua-fixtures-parallel.py --lua-version 5.4

  # Limit fixtures for quick testing
  python3 scripts/tests/run-lua-fixtures-parallel.py --limit 100 -v

  # Skip NovaSharp (only run reference Lua)
  python3 scripts/tests/run-lua-fixtures-parallel.py --skip-novasharp

  # Custom output directory
  python3 scripts/tests/run-lua-fixtures-parallel.py --output-dir artifacts/my-comparison
  ```
- **Output:** Results written to `artifacts/lua-comparison-results/` with per-fixture output files and `results.json` summary including elapsed time and throughput metrics.
- **expects-error handling:** Correctly handles `@expects-error: true` fixtures - non-zero exit code is treated as pass.

### run-lua-fixtures.sh (Legacy)

- **Purpose:** Sequential runner for Lua fixture files (slower but simpler).
- **Fixtures location:** `src/tests/WallstopStudios.NovaSharp.Interpreter.Tests/LuaFixtures/` (source-committed, generated by `tools/LuaCorpusExtractor/lua_corpus_extractor_v2.py`).
- **Usage:**
  ```bash
  # Run against Lua 5.4 (default)
  bash ./scripts/tests/run-lua-fixtures.sh

  # Run against specific Lua version
  bash ./scripts/tests/run-lua-fixtures.sh --lua-version 5.1

  # Skip incompatible fixtures (default, based on @lua-versions header)
  bash ./scripts/tests/run-lua-fixtures.sh --lua-version 5.2 --skip-incompatible

  # Run NovaSharp only (skip reference Lua)
  bash ./scripts/tests/run-lua-fixtures.sh --skip-lua

  # Limit number of snippets (for testing)
  bash ./scripts/tests/run-lua-fixtures.sh --limit 100 --verbose
  ```
- **Output:** Results written to `artifacts/lua-comparison-results/` with `<fixture>.lua<version>.{out,err,rc}` and `<fixture>.nova.{out,err,rc}` files plus `results.json` summary.

### compare-lua-outputs.py

- **Purpose:** Compare Lua execution outputs from reference Lua and NovaSharp with semantic normalization.
- **Usage:**
  ```bash
  # Compare after running fixtures
  python3 scripts/tests/compare-lua-outputs.py

  # Compare specific Lua version results

  python3 scripts/tests/compare-lua-outputs.py --lua-version 5.1 --results-dir artifacts/lua-comparison-results

  # Strict mode (exact match, no normalization)
  python3 scripts/tests/compare-lua-outputs.py --strict

  # Verbose output with diff details
  python3 scripts/tests/compare-lua-outputs.py --verbose
  ```
- **Normalizations applied:**
  - Removes NovaSharp CLI `[compatibility]` info lines
  - Normalizes floating-point precision (10 decimal places)
  - Replaces memory addresses with `<addr>`
  - Normalizes line numbers in error messages
  - Normalizes platform-specific paths
- **Output:** `comparison.json` with match/mismatch statistics and Markdown summary.

## Fixture Catalog

### update-fixture-catalog.ps1

- **Purpose:** Regenerates `src/tests/WallstopStudios.NovaSharp.Interpreter.Tests/FixtureCatalogGenerated.cs` so analyzers always see explicit references to every `[TestFixture]`. The generated file now records "No NUnit fixtures remain," but the script stays checked in so we can quickly resurrect NUnit coverage if a future project needs it.
- **Usage:** From the repo root run `pwsh ./scripts/tests/update-fixture-catalog.ps1`. The script accepts optional `-TestsRoot` and `-OutputPath` parameters when experimenting with other assemblies.
- **CI integration:** The interpreter NUnit project has been deleted, so the script no longer runs automatically. Execute it manually whenever you add/remove NUnit fixtures in any legacy branch to keep the generated file accurate.

## Test Runtime Comparison

### compare-test-runtimes.ps1

- **Purpose:** Captures timing deltas between two `dotnet test` invocations (historically NUnit vs. TUnit, now typically TUnit vs. TUnit or TUnit vs. archived NUnit logs) and stores the aggregate JSON under `artifacts/tunit-migration/<name>.json`.
- **Usage (current suites):** Provide the TUnit command via `-TUnitArguments` and, if the legacy NUnit project no longer exists, point `-BaselineArtefactPath` at a previously generated JSON file. The script replays the archived NUnit timing and only executes the TUnit run.
  ```powershell
  pwsh ./scripts/tests/compare-test-runtimes.ps1 `
      -Name remote-debugger-final `
      -BaselineArtefactPath artifacts/tunit-migration/remote-debugger-sample.json `
      -TUnitArguments @(
          "--project", "src/tests/WallstopStudios.NovaSharp.RemoteDebugger.Tests.TUnit/WallstopStudios.NovaSharp.RemoteDebugger.Tests.TUnit.csproj",
          "-c", "Release"
      )
  ```
- **Usage (legacy migration):** When a runnable NUnit project still exists, omit `-BaselineArtefactPath` and pass `-NUnitArguments @(...)`. The script runs both commands, logs detailed output under `artifacts/tunit-migration/tmp/<label>/`, and emits a JSON artefact summarizing per-test durations plus the total delta.

## Retired assets

- `NovaSharp.Parallel.runsettings` was removed after the final TUnit migration (2025-12-01). Microsoft.Testing.Platform already maxes out concurrency, so there is no longer a bespoke runsettings profile to pass to `dotnet test`.
